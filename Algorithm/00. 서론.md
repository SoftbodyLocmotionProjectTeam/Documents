# 알고리즘
소개문에서 설명하였듯이, Spinning Up AI는 상당히 수많은 알고리즘의 결합으로 이루어져있는데, Spinning Up AI를 쓰려면 대강적으로라도 알아야 한다고 권장하고 있다.

## 알고리즘 종류
~솔직히 나는 이 중에서 하나도 못 듣도 보도 못해 봤다. 미안하다...~

> 0. VPG (Vanilla Policy Gradient) 
> 0. TRPO (Trust Region Policy Optimization)
> 0. PPO (Proximal Policy Optimization)
> 0. DDPG (Deep Deterministic Policy Gradient)
> 0. TD3 (Twin Delayed DDPG)
> 0. SAC (Soft Actor-Critic)

## 특징
1. 위에 설명할 알고리즘들은 MLP(Multi-Layer Perceptron)<sup id="a1">[1](#b1)</sup> Actor-Critic <sup id="a2">[2](#b2)으로 구현했다. 
1. 이 알고리즘들이 어떻게 행동하는지, 전부 다 관측하기 좋게 만들었단다. 
1. 이미지 기반(컴퓨터 비전)에 기반한 강화 학습 환경이 아니다. Gym과 Mujoco 환경이 그 예. ( 설치 확인할 때 썻던 그 방법으로 강화 학습 양상을 보여주려나 보다.)

1. 스피닝 업은 TRPO를 제외하고는, 각 알고리즘들을 파이토치(PyTorch)<sup id="a3">[3](#b3)</sup>와 텐서플로 v1<sup id="a4">[4](#b4)</sup> 두 가지로 구현했다고 한다. ~역시 갓... 오픈 AI..~
1. TRPO는 현재 텐서플로에서만 쓸 수 있다. (대충 TRPO 할 수 있는 다수의 surrogate objective function 문제는 PRO 선에서 해결 가능하다고 한다.)


## 왜 이 알고리즘들로 구현했는가?
일단, 이 알고리즘들은 딥 강화학습에서 중추적인 역할을 하는 애들이다. 현재 쓰이는 딥 강화학습 영역의 역사 통틀어 아주 유용한 성과를 냈던 애들이기도 하다. 특히나 PPO나 SAC는 아주 정점을 찍은 애들인데, 신뢰성의 부분에서는 예술의 경지에 이른 애들이면서, 정책-학습 알고리즘에서도 효율성의 지표로 쓰이는 것들이다. 또한 이것들은 딥 강화학습 알고리즘 설계와 사용에 있어서도 트레이드-오프(어떤 것을 쓰면 무엇이 좋고 나쁜지)들을 개발자에게 보여주는 역할도 한다.

## 온 폴리시 알고리즘(On-Policy Algorithm)
### 종류
> VPG(Vanila Policy Gradient) : 바닐라 정책 그래디언트
> TRPO(Trust Region Policy Optimization) : 신뢰 구간 정책 최적화
> PPO(Proximal Policy Optimization) : 근위 정책 최적화

### 설명
VPG(Vanila Policy Gradient)는 가장 기본적이고, 딥 강화학습 입문 수준의 알고리즘이다. 이게 딥 강화 학습이 세상에 나타나기 위한 모든 것인 알고리즘이기 때문이다. VPG의 핵심적인 기여는 1980년대 말 ~ 1990년대 초로 거슬러 올라간다. VPG는 이후에 등장할 TRPO나 PPO와 같은 알고리즘의 탄생에 길을 터놓았던 알고리즘이라고 할 수 있다.

VPG를 이어서 만든 알고리즘들을 정책 상 알고리즘(on-Policy Algorithm)이라고 하는데, 이것은 샘플 효력<sup id="a5">[5](#b5)</sup>을 떨구는 기존 데이터를 쓰지 않는다. 직접적으로 정책의 성능에 맞게, 우리가 도달하고자 하는 목적 함수를 직접 최적화하고, 업데이트 계산에 필요한 모든 정책 상의 데이터를 수학적으로 계산한다.

### 샘플 효력과 안정성
정책 상의 알고리즘은 샘플 효력과 안정성의 측면에서 트레이드-오프를 보인다. 하지만, VPG부터 TRPO, PPO로 진화하면서 샘플 효력이 떨어져 가는 현상을 목격할 수 있다.

## 오프 폴리시 알고리즘(Off-Policy Algorithm)
### 종류
> DDPG
> TD3
> SAC

### 설명
DDPG(Deep Deterministic Policy Gradient)는 VPG와 유사하다. 하지만, DDPG의 모태가 되는 DPG 이론보다 훨씬 최신 기술인데도 2014년까지 쓰여지지 않았다. 일단 DDPG는 Q 러닝<sup id="a6">[6](#b6)</sup>에 아주 밀접하게 연관되어있는데 Q-함수와 폴리시를 동시에 업데이트해가면서 학습을 하기 때문이다.

DDPG나 Q 러닝 같은 친구들을 오프 폴리시 알고리즘이라고 한다. 온 폴리시와의 차이는 모델이 있냐 없냐가 될 것이다. 어쨌든, 얘네들은 샘플링한 옛날 데이터를 사골이 마르고 닳때까지 우려먹는다. 즉, 굉장히 효율적으로 쓴다는 말이다. 여기서 벨만의 방정식을 최적화면에서 극단적으로 잘 이용해 먹는데, 그게 어느 정도냐면 Q 함수가 어떤 환경에서도, 보상 값이 높은 구간에 대해 충분히 시뮬레이션을 돌리는다는 전제하에, 데이터가 엥간치 맞게 나온다.

그러나 문제점은 벨만 방정식 면에서 아주 잘 학습한다는게 정책 성능이 뛰어나다는 걸 보장을 못한다. 경험적으로 본다면 뛰어난 성능을 보이는 정책은 샘플 효력이 아주 좋다는 것이지만, 이러한 보장이 없는 경우, 알고리즘이 잠재적으로 대단히 불안정하다. TD3와 SAC는 DDPG의 다음 주자들인데, 이와 같은 논란들을 완화시키려고 아주 많은 시도를 한 성과이다.

## 코드 형태
Spinning Up의 모든 코드는 표준적인 템플릿에 붙어있다. 2개의 파일로 나뉘어지는데, 하나는 알고리즘 파일이고 하나는 코어 파일이다. 알고리즘 파일에는 알고리즘의 핵심적인 로직이 서술되어 있고, 코어 파일에는 알고리즘을 실행시키기 위한 다양한 유틸리티들이 서술되어 있다.

### 각주
###### <b id="b1">[1](#a1)</b> 다중 레이어 퍼셉트론 : 노영균 교수님 인공지능 수업시간에 다뤘던 그거 맞다. 앞으로 나타날 일을 계산에 의해 예측하고 피드백하는 뉴럴 네트워크 클래스로, 이 내용은 추후에 다뤄보자.  
<img src = "https://wikimedia.org/api/rest_v1/media/math/render/svg/167e8b5c38130ec92a2771bc384658772f387d02"/>

###### <b id="b2">[2](#a2)</b> 배우와 평론가 알고리즘 : 두 개의 뉴럴 네트워크 중 하나가 행동하고 다른 하나는 평가하는 알고리즘이다. 어떤 정책에 따라 해야하는 액션이 있을 때, 액터는 액션이 그 정책을 따르도록 파라미터를 업데이트하고, 평론가는 액터가 도출해낸 값에 따라 정책을 업데이트한다. 마찬가지로 추후에 다루도록 하자.

###### <b id="b3">[3](#a3)</b> 페이스북의 딥러닝 프레임워크다. 요즘 이게 쓰기가 더 쉬워서 개발자 사이에서 핫하고 한다. 또한 파이토치는 이름 그대로 파이썬을 스크립팅 언어로 사용(C++ 프레임워크에 파이썬 씌워 놓은게 아님. 근본 자체가 파이썬.)하며, 예전에 토치 프레임워크라는 것을 계승했다. 얘네는 속도를 극대화 하는 것을 목표로 했기 때문에, GPU 가속을 극대화 한다고 한다. ~아마 졸프가 끝날 무렵엔 GPU가 다 타들어 가있는 것 아닌가 모르겠다.~  파이토치의 큰 특징은 API의 즉시 실행, GPU 환경에서 텐서플로보다 빠르다, 각 반복 단계에서 그래프를 즉석으로 재생성하는 것이 있다. 파이토치는 동적이고 텐서플로우는 정적이다 라는 말이 있을 정도다. 이거 완전 교류전기 vs 직류전기 싸움 아니냐?

###### <b id="b4">[4](#a4)</b> 구글의 딥러닝 프레임워크다. 사실 구글에서 유튜브도 유튜브지만 앞으로의 구글의 돈줄은 텐서플로우와 TPU(Tensor Process Unit)이 될 것이라고 전문가들은 내다본다. 현재 머신러닝은 사람이 만드려는 것에 비해, 천문학적인 연산량에 인프라가 핵심적인데, 구글은 TPU를 인프라로서 거의 독점적으로 제공한다. 그런데 이용료가 연 억단위, 그 이상에 육박한다. 그리고 흥미로운 것은, 파이토치와 텐서플로, 세계 최강자들의 싸움 페이스북과 구글의 경쟁인데, 두 회사의 창립자, 마크 주커버그와 래리 페이지 둘 다 유대인들이다. 정말 신의 후예들인 것 같기도 하다. 어쨋든 논문에서도 가장 많이 인용되는 프레임워크이며, 머신러닝 발달에 가장 큰 기여를 한 프레임워크라고 해도 과언이 아니다. 단점이 v 1.0이 배우는 것도 무진장 힘들고 C++ 프레임워크라서 속도 올릴려면, 프레임워크를 뜯어야 한다. 그리고 1.0v에 한해서 파이토치보다 느리다.

###### <b id="b5">[5](#a5)</b> 샘플로부터, 얼마나 많은 학습을 할 수 있는지를 샘플 효력(Sample Efficiency)라고 한다. 가령, 이세돌과 바둑을 몇 판 둔 인공지능이 바둑계를 정복한다면, 굉장히 Sample - Efficient라고 할 수 있다.

###### <b id="b6">[6](#a6)</b> Q 러닝(Q-learning)은 모델 없이 학습하는 강화 학습 기법 가운데 하나이다. Q 러닝은 주어진 유한 마르코프 결정 과정의 최적의 정책을 찾기 위해 사용할 수 있다. Q 러닝은 주어진 상태에서 주어진 행동을 수행하는 것이 가져다 줄 효용의 기대값을 예측하는 함수인 Q 함수를 학습함으로써 최적의 정책을 학습한다. 
